Project Q-Manifold: A Hybrid Quantum-Classical Framework for Hyperbolic Metric Refinement of Semantic Embeddings via Primitives V2 Batch Execution1. Executive Summary: The Convergence of Geometric Deep Learning and Quantum ProcessingThe evolution of Natural Language Processing (NLP) has historically been driven by the scaling hypothesis—the empirical observation that increasing parameter counts and training data volumes yields predictable gains in model performance. However, recent investigations into the intrinsic geometry of these massive models have begun to reveal a counter-narrative: beneath the high-dimensional surface of billion-parameter models lies a surprisingly low-dimensional fundamental structure. This report synthesizes a "Trilogy" of recent research findings—spanning the discovery of universal intrinsic dimensionality, the task-asymmetric nature of compression, and preliminary quantum hardware validations—to propose a novel research vector designated as "Path B: Quantum Metric Refinement."The prevailing orthodoxy in Quantum Natural Language Processing (QNLP) has largely focused on using quantum circuits as primary compression engines, attempting to map high-dimensional classical data directly into logarithmic-scale Hilbert spaces. The synthesis of the attached documentation dismantles this approach for generative tasks while simultaneously illuminating a more sophisticated role for quantum hardware: not as a compressor, but as a metric refiner.This report posits that classical linear methods (Principal Component Analysis) are theoretically optimal for the "bulk" compression of semantic spaces down to their intrinsic dimensionality ($\sim$20D), as proven in the foundational literature.1 The quantum processor is then tasked with a subtler, higher-value operation: "bending" this linear 20D manifold to better approximate the non-Euclidean, hyperbolic geometry inherent in semantic relationships. By leveraging the unique topological properties of Hilbert space and the noise-assisted optimization capabilities of near-term hardware (IBM Quantum ibm_fez), we aim to construct a hybrid architecture where classical CPUs handle dimensionality reduction and quantum QPUs handle geometric fidelity.The proposed experimental framework is rigorously designed to operate within the strict constraints of the IBM Quantum Free Tier, utilizing the Primitives V2 Batch Execution Mode to overcome the latency bottlenecks of interactive sessions. This document serves as a comprehensive research definition, theoretical justification, and execution roadmap for realizing this hybrid quantum-classical architecture.2. The Theoretical Foundation: A Trilogy of DiscoveryThe formulation of the "Quantum Metric Refinement" hypothesis is not arbitrary; it is the logical conclusion of a rigorous, three-phase investigatory process that has systematically mapped the capabilities and limitations of both classical and quantum embedding spaces.2.1 Phase I: The Universal Geometry of Semantic KnowledgeThe first pillar of this research 1 fundamentally alters our understanding of how Large Language Models (LLMs) represent meaning. Through a massive cross-model analysis involving 19 distinct transformer architectures—ranging from the 11.7 million parameter ALBERT to the 7 billion parameter Mistral-7B—researchers established a "Universal Intrinsic Dimensionality" for semantic concepts.2.1.1 The ~20D ConstantThe central finding is that despite the vast differences in ambient space (768 dimensions for BERT, 4096 for Mistral), the intrinsic semantic manifold occupies approximately $19.0 \pm 4.2$ dimensions. This figure appears to be a universal constant of semantic representation, persisting across:Architectural Paradigms: Decoder-only (GPT-2), Encoder-only (BERT), and Encoder-Decoder (T5) models all converge on this dimensionality.Scale: The structure remains invariant across a 636-fold increase in parameter count.Time: Models developed between 2018 and 2024 show no significant drift in this intrinsic value, suggesting it reflects a constraint of language itself rather than model design.1This finding implies that the vast majority of the embedding space in modern LLMs is topologically empty or redundant regarding semantic similarity. It allows for compression ratios averaging 73.5:1 and reaching an unprecedented 177:1 for Mistral-7B, significantly outperforming dedicated compression techniques like PCA-RAG or Matryoshka Representation Learning (MRL).12.1.2 The Local-Global Duality and Scale ParadoxA critical nuance in this discovery is the Local-Global Duality.1 The research identified a consistent 8-10x ratio between the global dimensionality (captured by PCA, approx. 180D) and the local dimensionality (captured by Maximum Likelihood Estimation, approx. 20D). This structure reveals a hierarchical manifold organization: simple local neighborhoods, where synonymy and immediate semantic relations live, are low-dimensional ($\sim$20D), but these local patches are embedded within a more complex, curved global space.Furthermore, the Scale Paradox observed in the data challenges the assumption that larger models are more complex. The correlation between model size and local compression is positive ($\rho=0.52$), meaning larger models actually learn more efficient local representations. They achieve "richness without complexity," packing more semantic precision into the same low-dimensional manifold structure.12.2 Phase II: Task Asymmetry and the "Perplexity Catastrophe"The second pillar 1 introduces a necessary constraint by proving that compression tolerance is highly task-dependent. While the ~20D manifold is sufficient for retrieval, it is woefully inadequate for generation.2.2.1 The Retrieval vs. Generation DichotomyThe research empirically validates a sharp phase transition in utility:Retrieval (Success): Compressing embeddings to ~20D maintains 86.2–94.9% of retrieval quality compared to the uncompressed baseline. This confirms that semantic similarity is a manifold property preserved under projection.1Generation (Failure): The same compression results in 0% quality for generative tasks. The study terms this the "Perplexity Catastrophe," quantified by a power law $P(r) \approx 26.1 \times r^{0.80}$, where perplexity degrades exponentially with compression ratio.12.2.2 The Failure of Quantum Compression for GenerationCrucially for our current roadmap, this phase evaluated "Quantum-Inspired Hilbert Space Projections" as a primary compression mechanism for generative tasks. The results were damning: the quantum-inspired method yielded a 4,660x worse perplexity than classical PCA.1This negative result is foundational. It proves that quantum circuits are distinctively poor at the "bulk compression" required to preserve the high-frequency token statistics needed for text generation. It suggests that the Universal Information Bottleneck cannot be bypassed by quantum encoding.1 However, the same study confirmed that for retrieval tasks, linear PCA is optimal, outperforming non-linear methods like UMAP, which degraded performance by 30.9%.1 This suggests the global semantic space is largely linear, but requires refinement—precisely the niche our hybrid architecture will fill.2.3 Phase III: The Hardware Advantage AnomalyThe third pillar 1 provides the empirical spark for the "Metric Refinement" hypothesis. In a "Hyperbolic Contrastive" experiment executed on IBM Quantum hardware (ibm_torino), a parameterized quantum circuit demonstrated a surprising superiority over classical simulation.2.3.1 The Hardware Optimization AdvantageThe experiment trained a Siamese quantum network to align quantum fidelity with hyperbolic distance. The key result was that the hardware execution achieved a loss of 0.012, which was 67.9% better than the noiseless simulator (loss 0.039).1This counter-intuitive finding—that a noisy quantum processor outperformed a perfect mathematical simulation—suggests that real quantum hardware possesses intrinsic properties beneficial for geometric optimization. Whether due to noise-induced smoothing of the loss landscape (preventing local minima entrapment) or the specific topological connectivity of the heavy-hex lattice, the hardware proved to be a superior optimizer for this specific geometric alignment task.2.3.2 The Overfitting TrapHowever, this success was tempered by a critical methodological flaw: the model was trained on a single concept pair. While it learned to align that specific pair perfectly, it likely "memorized" the rotation rather than learning a generalized geometric transformation.1 Solving this overfitting issue via Batch Contrastive Learning is the primary objective of the experimental design proposed in Section 4.2.4 Synthesis: The "Quantum Metric Refinement" HypothesisThe convergence of these three findings leads to the formulation of our core hypothesis. We do not ask the quantum computer to compress data (Paper 2 proves it is inefficient at this). We do not ask the classical computer to handle fine-grained topology (Paper 1 implies local curvature is complex).Instead, we propose a Hybrid Quantum-Classical Architecture:Classical PCA (The Compressor): Leverages the findings of Paper 1 and 2 to efficiently reduce the ambient embedding space (4096D) to the intrinsic dimension (20D). This removes noise and redundancy, providing a high-density signal.Quantum Circuit (The Refiner): Accepts the 20D vectors and applies a parameterized unitary transformation. Its goal is to "bend" the Euclidean PCA space into a geometry that respects the Hyperbolic nature of semantic hierarchies.We hypothesize that the quantum circuit, operating in a high-dimensional Hilbert space, can induce non-Euclidean metrics (via entanglement and interference) that are mathematically inaccessible or computationally expensive for classical linear methods, thereby refining the retrieval precision for hierarchical concepts.3. The Geometry of Semantic Space: Why "Refinement" MattersTo understand the mechanism of the proposed research, we must delve into the geometric mismatch that currently limits classical NLP embeddings and how quantum mechanics offers a solution.3.1 The Euclidean vs. Hyperbolic ConflictStandard LLM embeddings (and PCA projections) exist in Euclidean space ($\mathbb{R}^n$). In this geometry, the volume of space expands polynomially with radius ($r^n$). However, semantic networks—defined by hyponymy/hypernymy relationships (e.g., "Animal" $\rightarrow$ "Mammal" $\rightarrow$ "Dog")—expand exponentially. This is the structure of a tree or a complex network.Embedding a tree-like structure into a flat Euclidean space inevitably causes distortion. To fit the exponentially growing number of nodes into a polynomial space, nodes must be crushed together, losing the distinctiveness of their relationships. This is known as the "Distortion Problem" in geometric deep learning.2Hyperbolic space (specifically the Poincaré disk model) has negative curvature and expands exponentially, providing a natural "home" for hierarchical data. In hyperbolic space, the distance between points increases as they move toward the boundary, allowing for the conflict-free representation of vast taxonomies.43.2 Quantum Hilbert Space as a GeometryQuantum states live in Hilbert space, a complex vector space equipped with an inner product. While formally linear, the projective Hilbert space (the space of physical states, where global phase is ignored) has a complex geometry (Fubini-Study metric) that shares curvature properties with non-Euclidean spaces.6Furthermore, entanglement creates non-local correlations that can be interpreted as "wormholes" or connections in the geometry, effectively altering the distance metric between points in a way that classical Euclidean metrics cannot easily replicate.3.3 The Refinement MechanismIn our proposed architecture, the quantum circuit acts as a Metric Learning Kernel.Input: Two vectors $x, y$ in 20D Euclidean space (PCA reduced).Transformation: A Parameterized Quantum Circuit (PQC) $U(\theta)$ maps these to quantum states $|\psi_x\rangle, |\psi_y\rangle$.Target: We train $U(\theta)$ such that the Quantum Fidelity $|\langle \psi_x | \psi_y \rangle|^2$ approximates the Hyperbolic Distance $d_{Hyp}(x, y)$ rather than the Euclidean distance.Essentially, we are using the quantum processor to simulate a hyperbolic manifold for the embeddings. The "Refinement" lies in the circuit's ability to learn a transformation $\theta$ that stretches and compresses the 20D input space to match the ground-truth semantic curvature, correcting the distortion introduced by the linear PCA compression.4. Hardware Constraints and Architectural DecisionsExecuting this vision on real hardware requires navigating the specific constraints of the IBM Quantum ecosystem, particularly the shift to Qiskit Runtime Primitives V2. This section details the technical decisions necessitated by the hardware "Free Tier" access and the architecture of the 127+ qubit Eagle/Heron processors.4.1 The Shift to Batch Execution (Primitives V2)The most significant operational constraint is the inability to use Session mode for interactive training. Sessions provide dedicated access and caching but are restricted or resource-intensive. We must use Batch Mode.4.1.1 Batch Mode MechanicsIn Batch Mode, jobs are optimized for throughput but do not guarantee exclusive lock on the QPU. Crucially, the classical optimization loop (measuring, calculating gradient, updating parameters) cannot happen "inside" the QPU reservation.Implication: We cannot do standard step-by-step Gradient Descent where step $N+1$ waits for step $N$.Solution: We must adopt a Mini-Batch SPSA approach (detailed in Section 5). We must bundle multiple circuit evaluations (for gradient estimation across multiple data points) into a single job submission.84.2 Circuit Transpilation and ISAQiskit Runtime V2 enforces a strict requirement: circuits submitted to the primitives must be ISA (Instruction Set Architecture) circuits.10Virtual to Physical: We cannot submit abstract circuits. We must transpile them to the specific hardware graph (e.g., ibm_fez) before submission.Layout & Routing: The transpilation must respect the limited connectivity of the "Heavy Hex" lattice used in IBM's Eagle processors. This means explicit SWAP gates will be inserted if we entangle non-adjacent qubits.Optimization Level: We will use optimization_level=3 in the generate_preset_pass_manager to minimize depth and noise, which is critical for fidelity preservation.104.3 The Necessity of Angle EncodingThe choice of Angle Encoding is dictated by both theoretical findings and hardware limitations.Paper 2 Constraint: Quantum compression fails when handling high dimensions directly. We are strictly inputting 20 dimensions.Hardware Constraint: The initialize instruction (needed for Amplitude Encoding) decomposes into $O(2^N)$ gates, creating a circuit depth that far exceeds the coherence time of NISQ devices.Angle Encoding Advantage: It requires only a single layer of $R_Y$ rotations ($O(1)$ depth). While it uses more qubits (20 qubits for 20 features), ibm_fez has 156 qubits, making qubit count a non-issue. The constraint is depth, not width.124.4 Architecture Selection: RealAmplitudesWe select the RealAmplitudes ansatz for the variational layer.Structure: Alternating layers of $R_Y$ rotations (trainable weights) and $CX$ entanglement.Justification: It produces real-valued amplitudes, which aligns with the real-valued nature of the PCA embeddings and the cosine similarity metric. It is "Hardware Efficient," meaning the entangling gates can be aligned with the native coupling map of the device.15Entanglement Map: We will use a Circular entanglement map. This topology induces non-local correlations that travel across the qubit register, mimicking the interconnected nature of semantic graphs better than simple linear (nearest-neighbor) entanglement.165. Experimental Design: The Q-Manifold ProtocolThis section details the precise experimental protocol for Batch Contrastive Learning. It translates the theoretical roadmap into an executable plan compatible with Qiskit SDK v1.x and Runtime V2.5.1 Phase 1: Classical Data PreparationBefore engaging the QPU, the data must be rigorously pre-processed to match the $\sim$20D intrinsic dimensionality finding.Dataset Construction:We will generate a contrastive dataset derived from WordNet hierarchies or a similar semantic graph, aligned with the Mistral-7B token space.Source: Extract embeddings for 1000 concept words from Mistral-7B (Layer 31).Compression: Train a PCA model on the full set and reduce all vectors to 20 dimensions.Normalization:Crucial Step: Quantum rotation gates $R_Y(\theta)$ are $2\pi$-periodic. Input data must be scaled to avoid phase wrapping (where very different values map to the same state).Scaling: Apply MinMaxScaler to map the 20D vectors into the range $[0.1, \pi - 0.1]$. We avoid $0$ and $\pi$ to keep the states away from the poles of the Bloch sphere, where gradients vanish.12Pair Generation:Create triplets $(A, P, N)$ where $A$ is an anchor, $P$ is a positive match (synonym/hypernym), and $N$ is a negative match (random concept).Ground Truth: Compute the Poincaré Distance for all pairs $(A,P)$ and $(A,N)$ using the formula:$$d_{Poincaré}(u, v) = \text{arccosh}\left( 1 + 2 \frac{\|u-v\|^2}{(1-\|u\|^2)(1-\|v\|^2)} \right)$$This pre-computed distance serves as the target label for the quantum training.55.2 Phase 2: Quantum Circuit Design (20 Qubits)The circuit must map two classical vectors $\vec{x}$ and $\vec{y}$ to a fidelity score. We employ a Siamese Network architecture implemented as a single "Compute-Uncompute" circuit.The Circuit Sequence:State Preparation (Input X): Apply $U_{enc}(\vec{x})$ using Angle Encoding ($R_Y$ gates).$$|\psi_x\rangle = \bigotimes_{i=0}^{19} R_Y(x_i) |0\rangle$$Variational Transformation: Apply the trainable ansatz $V(\vec{\theta})$.$$|\phi_x\rangle = V(\vec{\theta}) |\psi_x\rangle$$Inverse Transformation: Apply the inverse ansatz $V^\dagger(\vec{\theta})$.Inverse Preparation (Input Y): Apply the inverse encoding $U_{enc}^\dagger(\vec{y})$.Measurement: Measure the probability of the all-zero state $|0\rangle^{\otimes 20}$.Mathematical Justification:The probability of measuring $|00...0\rangle$ corresponds to the squared overlap (fidelity) between the processed states:$$ P(0) = |\langle 0 | U_{enc}^\dagger(\vec{y}) V^\dagger(\vec{\theta}) V(\vec{\theta}) U_{enc}(\vec{x}) | 0 \rangle|^2 = |\langle \phi_y | \phi_x \rangle|^2 $$If the circuit $V(\theta)$ successfully maps $x$ and $y$ to the same point in Hilbert space, the fidelity is 1. If they are orthogonal, it is 0.ISA Implementation Details:Backend: ibm_fez (156 qubits).Qubits Used: 20.Transpilation: The circuit must be explicitly transpiled to the backend's coupling map using generate_preset_pass_manager(backend=backend, optimization_level=3). This ensures all 2-qubit gates are physically executable.115.3 Phase 3: The Batch-Mode Optimization Loop (SPSA)This is the core operational innovation required to bypass the "Session" limitation. We utilize Simultaneous Perturbation Stochastic Approximation (SPSA) adapted for Mini-Batch Asynchronous Execution.The Challenge: SPSA typically requires two measurements ($\theta+\epsilon$ and $\theta-\epsilon$) to estimate the gradient for one step. In standard SGD, we would do this for one data point, update, and repeat. In Batch Mode, we cannot wait.The Solution: Parameter Broadcasting in PUBsQiskit Runtime V2 introduces Primitive Unified Blocs (PUBs), which allow a single circuit to be executed with multiple parameter sets simultaneously.17The Algorithm:Sample Batch: Select a mini-batch of $B=32$ data pairs $\{(x_k, y_k)\}_{k=1}^{32}$.Generate Perturbation: Generate a random perturbation vector $\Delta_k$ (Bernoulli distribution $\pm 1$).Construct Parameter Sets: For each data pair $k$, we need to evaluate the circuit at two points:$\Theta_+ = \theta_{current} + c_k \Delta_k$$\Theta_- = \theta_{current} - c_k \Delta_k$Broadcasting: We construct a single PUB: (ISA_Circuit,).The Bindings array will have shape $(32 \times 2, \text{num_params})$. It contains the concatenated vectors $$ for all 32 pairs and both shift directions.Submit Batch Job: We send one sampler.run() request containing these 64 evaluations.Gradient Estimation:Upon receiving the results (frequencies of '0' state), we compute the approximate gradient for each pair:$$\hat{g}_k(\theta) = \frac{\text{Loss}(\Theta_+) - \text{Loss}(\Theta_-)}{2 c_k} \Delta_k$$We then average these gradients: $\hat{g} = \frac{1}{B} \sum \hat{g}_k$.Update: Update parameters $\theta_{new} = \theta_{old} - \alpha \cdot \hat{g}$.This approach turns the latency of the cloud queue into a throughput advantage. By bundling 64 circuit executions into one job, we maximize the utility of the Batch mode while adhering to the stochastic nature of SPSA.85.4 Loss Function: Hyperbolic Contrastive AlignmentThe loss function must explicitly penalize deviations from the hyperbolic metric.$$\mathcal{L} = \text{MSE}\left( \text{Fidelity}(x,y), \text{Sim}_{Poincaré}(x,y) \right)$$Where $\text{Sim}_{Poincaré}$ is a normalized similarity score derived from the Poincaré distance (e.g., via a Gaussian kernel $e^{-d^2}$). This forces the quantum geometry to become isometric to the hyperbolic geometry of the data.6. Implementation Strategy: Qiskit Code AnalysisThe following Python implementation plan demonstrates the practical application of the design, specifically handling the transition to SamplerV2 and Batch contexts.6.1 Setup and Circuit ConstructionPython# -----------------------------------------------------------------------------
# Q-MANIFOLD: Batch Contrastive Learning Implementation
# -----------------------------------------------------------------------------
import numpy as np
from qiskit import QuantumCircuit, transpile
from qiskit.circuit import ParameterVector
from qiskit.circuit.library import RealAmplitudes
from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager
from qiskit_ibm_runtime import QiskitRuntimeService, Batch, SamplerV2 as Sampler

# 1. DEFINITIONS
NUM_QUBITS = 20  # From Paper 1 (20D Intrinsic Dim)
BACKEND_NAME = 'ibm_fez' 
BATCH_SIZE = 32
ITERATIONS = 50

# 2. QUANTUM CIRCUIT (Siamese Compute-Uncompute)
def build_circuit(n_qubits):
    # Parameter Vectors
    x = ParameterVector('x', n_qubits)       # Input 1
    y = ParameterVector('y', n_qubits)       # Input 2
    # Ansatz uses internal parameters we will extract later
    
    # Feature Map (Angle Encoding - RY only)
    fm = QuantumCircuit(n_qubits)
    for i in range(n_qubits):
        fm.ry(x[i], i)
        
    # Ansatz (RealAmplitudes - Hardware Efficient)
    # Reps=2 to balance expressivity with noise depth
    ansatz = RealAmplitudes(n_qubits, reps=2, entanglement='circular')
    theta = ansatz.parameters # Trainable weights
    
    # Composite Circuit
    qc = QuantumCircuit(n_qubits)
    qc.compose(fm, inplace=True)                 # Encode X
    qc.compose(ansatz, inplace=True)             # V(theta)
    qc.compose(ansatz.inverse(), inplace=True)   # V_dag(theta)
    
    # Inverse Feature Map for Y (using Y params)
    fm_y_inv = QuantumCircuit(n_qubits)
    for i in range(n_qubits):
        fm_y_inv.ry(y[i], i)
    fm_y_inv = fm_y_inv.inverse()
    
    qc.compose(fm_y_inv, inplace=True)           # Encode_dag Y
    qc.measure_all()
    
    return qc, theta, x, y

qc_raw, params_theta, params_x, params_y = build_circuit(NUM_QUBITS)

# 3. ISA TRANSPILATION
service = QiskitRuntimeService()
backend = service.backend(BACKEND_NAME)
pm = generate_preset_pass_manager(backend=backend, optimization_level=3)
isa_circuit = pm.run(qc_raw) 
# isa_circuit is now physically routed to ibm_fez topology
6.2 The Batch Optimization LoopThis section implements the Mini-Batch SPSA logic. Note the specific handling of SamplerV2 results, which return BitArrays rather than quasi-probabilities. We must calculate the frequency of the zero-state manually or use the get_counts() helper.Python# 4. BATCH SPSA OPTIMIZER
def run_optimization_step(current_weights, data_batch, targets, batch_context):
    """
    Executes one SPSA step using a single Batch Job submission.
    """
    # SPSA Hyperparameters
    c = 0.1  # Perturbation magnitude
    a = 0.1  # Learning rate
    
    # Perturbation Vector (Bernoulli +/- 1)
    delta = np.random.choice([-1, 1], size=len(current_weights))
    
    weights_plus = current_weights + c * delta
    weights_minus = current_weights - c * delta
    
    # Construct PUB Bindings (Broadcasting)
    # We need to bind [x, theta, y] for every sample in batch
    # Order in ISA circuit: likely [x..., theta..., y...] but we must match
    # isa_circuit.parameters order. 
    
    # (Simplified binding logic for report clarity)
    bindings_plus =
    bindings_minus =
    
    for i in range(BATCH_SIZE):
        x_i, y_i = data_batch[i]
        # Bind x_i, weights_plus, y_i
        # Bind x_i, weights_minus, y_i
        # Append to lists...
    
    # COMBINE all into one list for the PUB
    # Shape: (2 * BATCH_SIZE, Num_Params)
    all_bindings = bindings_plus + bindings_minus
    
    # SUBMIT JOB
    sampler = Sampler(mode=batch_context)
    # One PUB: (Circuit, Bindings)
    job = sampler.run([(isa_circuit, all_bindings)])
    result = job.result() 
    
    # PROCESS RESULTS (Gradient Calculation)
    # SamplerV2 returns data per PUB. 
    pub_result = result 
    # Extract counts. data.meas is the register name
    counts = pub_result.data.meas.get_counts() 
    
    grad_accum = 0
    total_loss = 0
    
    for i in range(BATCH_SIZE):
        # Retrieve counts for + and - perturbations
        counts_p = counts[i]
        counts_m = counts
        
        # Calculate Fidelity: Prob(00...0)
        shots = sum(counts_p.values())
        fid_p = counts_p.get('0'*NUM_QUBITS, 0) / shots
        fid_m = counts_m.get('0'*NUM_QUBITS, 0) / shots
        
        # Loss = (Fidelity - Target)^2
        loss_p = (fid_p - targets[i])**2
        loss_m = (fid_m - targets[i])**2
        
        # SPSA Gradient Estimator
        # g = (L+ - L-) / (2c) * delta
        grad_i = (loss_p - loss_m) / (2 * c) * delta
        grad_accum += grad_i
        total_loss += (loss_p + loss_m) / 2
        
    # Average Gradient
    avg_grad = grad_accum / BATCH_SIZE
    
    # Update Weights
    new_weights = current_weights - a * avg_grad
    
    return new_weights, total_loss / BATCH_SIZE

# Execution Context
weights = np.random.rand(len(params_theta))
with Batch(backend=backend) as batch:
    for i in range(ITERATIONS):
        # Load batch...
        weights, loss = run_optimization_step(weights, batch_data, batch_targets, batch)
        print(f"Iter {i}: Loss {loss}")
6.3 Critical Considerations for Primitives V2DataBin Attributes: In SamplerV2, the results are accessed via job.result().data.<register_name>. If the circuit measurement register is named meas, we access data.meas.10Shots vs Precision: Unlike Estimator, SamplerV2 uses shots. We should set default_shots=1024 or higher in the Sampler options to ensure statistical significance for the fidelity measurement, as we are looking for a specific bitstring ('0...0') frequency.10Error Mitigation: Primitives V2 automatically apply techniques like Dynamical Decoupling and Pauli Twirling if configured. We should enable these in the options dictionary to stabilize the fidelity measurements against coherent noise.217. Conclusion and Expected OutcomesThe "Q-Manifold" project represents a decisive pivot from the brute-force "Quantum Compression" paradigm toward a nuanced "Quantum Metric Refinement" strategy. By synthesizing the discoveries of Universal Intrinsic Dimensionality (Paper 1) and the Task-Asymmetric nature of compression (Paper 2), we have isolated the optimal operating point for quantum hardware in the NLP pipeline: the 20-dimensional semantic manifold.7.1 Addressing the Research GapThis experimental design directly addresses the failures identified in the previous phases:Overcomes the Perplexity Catastrophe: By accepting that quantum circuits should not handle generation (which requires full-rank distributions), we focus entirely on retrieval, where the manifold assumption holds.1Solves the Overfitting Issue: The transition to Batch-Mode Stochastic SPSA ensures that the circuit learns a generalized geometric transformation applicable to the entire semantic space, rather than memorizing a single pair as occurred in Paper 3.1Respects Hardware Limits: The exclusive use of Angle Encoding and ISA Transpilation ensures the experiment is physically executable on IBM's utility-scale processors, avoiding the depth explosions associated with amplitude encoding.127.2 Predicted MetricsWe anticipate the following outcomes from the execution of this roadmap:Correlation > 0.8: A successful refinement will be evidenced by a high Pearson correlation between the quantum fidelity of test pairs and their ground-truth Poincaré distances.Retrieval Improvement: When applied to a retrieval benchmark, the quantum-refined embeddings should yield a higher Mean Reciprocal Rank (MRR) than the raw 20D PCA vectors, particularly for concepts deep in the WordNet hierarchy where Euclidean distortion is highest.By treating the quantum computer as a "geometric co-processor" rather than a "compression engine," this research establishes a viable path for QNLP in the NISQ era—one that leverages the inherent strengths of quantum mechanics to refine, rather than replace, classical understanding.Table 1: Summary of Methodological ShiftsFeatureOld Paradigm (Paper 2/3)New Paradigm (Q-Manifold)ReasonRole of QuantumCompression (4096D -> Low D)Metric Refinement (20D -> 20D)Paper 2 proved quantum compression fails; Paper 1 proved 20D is sufficient.DimensionalityVariable / HighFixed 20 (Intrinsic)Paper 1 established 20D as the universal constant.GeometryEuclidean / UndefinedHyperbolic (Poincaré)Semantic trees require exponential space; Quantum Hilbert space offers curvature.OptimizationSingle-Pair / InteractiveBatch-Mode SPSAPaper 3 showed overfitting; Hardware requires Batch mode for throughput.EncodingAmplitude / HybridAngle Encoding ($R_Y$)Amplitude encoding depth ($O(2^N)$) is infeasible on NISQ hardware.Target MetricReconstruction ErrorFidelity-Distance CorrelationGenerative reconstruction fails (Paper 2); Similarity correlation succeeds.